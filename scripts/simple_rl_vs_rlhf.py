#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ RL vs RLHF å¯¹æ¯”è„šæœ¬
ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œé¿å…ç½‘ç»œä¾èµ–
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
import time

from src.utils.training_utils import get_device_info


class SimpleTextDataset(Dataset):
    """ç®€å•çš„æ–‡æœ¬æ•°æ®é›†"""
    
    def __init__(self, num_samples=200, seq_length=10):
        self.num_samples = num_samples
        self.seq_length = seq_length
        
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬æ•°æ®
        self.texts = []
        for i in range(num_samples):
            # ç”Ÿæˆéšæœºæ–‡æœ¬åºåˆ—
            text = np.random.randint(0, 100, seq_length)
            self.texts.append(text)
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        return {
            'input_ids': torch.tensor(text, dtype=torch.long),
            'labels': torch.randint(0, 2, (1,)).item()
        }


class SimpleClassifier(nn.Module):
    """ç®€å•çš„åˆ†ç±»å™¨"""
    
    def __init__(self, vocab_size=100, hidden_dim=64, num_classes=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        self.classifier = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, input_ids):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        pooled = torch.mean(lstm_out, dim=1)
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)
        return logits


class SimplePolicyNetwork(nn.Module):
    """ç®€å•çš„ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, base_model, action_dim=2):
        super().__init__()
        self.base_model = base_model
        hidden_dim = 64  # ä¸base_modelåŒ¹é…
        
        # ç­–ç•¥å¤´
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, input_ids):
        # è·å–åŸºç¡€æ¨¡å‹çš„éšè—è¡¨ç¤º
        embedded = self.base_model.embedding(input_ids)
        lstm_out, _ = self.base_model.lstm(embedded)
        features = torch.mean(lstm_out, dim=1)
        
        # ç­–ç•¥å’Œä»·å€¼
        policy_logits = self.policy_head(features)
        value = self.value_head(features)
        
        return policy_logits, value


class SimpleRewardModel(nn.Module):
    """ç®€å•çš„å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        hidden_dim = 64
        
        self.reward_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, input_ids):
        embedded = self.base_model.embedding(input_ids)
        lstm_out, _ = self.base_model.lstm(embedded)
        features = torch.mean(lstm_out, dim=1)
        reward = self.reward_head(features)
        return reward


class PureRLTrainer:
    """å•çº¯çš„RLè®­ç»ƒå™¨ï¼ˆæ— äººç±»åé¦ˆï¼‰"""
    
    def __init__(self, policy_model, lr=3e-4, device="cpu"):
        self.policy_model = policy_model
        self.device = device
        self.optimizer = optim.Adam(policy_model.parameters(), lr=lr)
        
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.policy_model.to(device)
    
    def train_step(self, states, actions, rewards):
        """è®­ç»ƒæ­¥éª¤"""
        # ç­–ç•¥æ¨¡å‹å‰å‘ä¼ æ’­
        policy_logits, _ = self.policy_model(states)
        
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        dist = torch.distributions.Categorical(logits=policy_logits)
        log_probs = dist.log_prob(actions)
        
        # æŸå¤±å‡½æ•°ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
        loss = -(log_probs * rewards).mean()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'reward': rewards.mean().item(),
            'entropy': dist.entropy().mean().item()
        }


class SimpleRLHFTrainer:
    """ç®€åŒ–çš„RLHFè®­ç»ƒå™¨"""
    
    def __init__(self, policy_model, reward_model, reference_model, lr=1e-4, beta=0.1, device="cpu"):
        self.policy_model = policy_model
        self.reward_model = reward_model
        self.reference_model = reference_model
        self.beta = beta
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(policy_model.parameters(), lr=lr)
        
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.policy_model.to(device)
        self.reward_model.to(device)
        self.reference_model.to(device)
        
        # å†»ç»“å‚è€ƒæ¨¡å‹
        for param in self.reference_model.parameters():
            param.requires_grad = False
    
    def compute_kl_penalty(self, policy_logits, reference_logits):
        """è®¡ç®—KLæ•£åº¦æƒ©ç½š"""
        policy_dist = torch.distributions.Categorical(logits=policy_logits)
        reference_dist = torch.distributions.Categorical(logits=reference_logits)
        
        # ä½¿ç”¨KLæ•£åº¦
        kl_div = torch.distributions.kl.kl_divergence(policy_dist, reference_dist)
        
        return kl_div.mean()
    
    def train_step(self, states, actions, rewards):
        """è®­ç»ƒæ­¥éª¤"""
        # ç­–ç•¥æ¨¡å‹å‰å‘ä¼ æ’­
        policy_logits, _ = self.policy_model(states)
        
        # å‚è€ƒæ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            reference_logits, _ = self.reference_model(states)
        
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        dist = torch.distributions.Categorical(logits=policy_logits)
        log_probs = dist.log_prob(actions)
        
        # è®¡ç®—KLæ•£åº¦æƒ©ç½š
        kl_penalty = self.compute_kl_penalty(policy_logits, reference_logits)
        
        # æ€»æŸå¤±
        loss = -(log_probs * rewards).mean() + self.beta * kl_penalty
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'kl_penalty': kl_penalty.item(),
            'reward': rewards.mean().item()
        }


def create_simple_dataset():
    """åˆ›å»ºç®€å•çš„æ•°æ®é›†"""
    print("åˆ›å»ºç®€å•æ•°æ®é›†...")
    
    train_dataset = SimpleTextDataset(num_samples=200, seq_length=10)
    val_dataset = SimpleTextDataset(num_samples=50, seq_length=10)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    return train_loader, val_loader


def train_pure_rl(base_model, train_loader, device, epochs=3):
    """è®­ç»ƒå•çº¯çš„RLæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("è®­ç»ƒå•çº¯çš„RLæ¨¡å‹ï¼ˆæ— äººç±»åé¦ˆï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy_model = SimplePolicyNetwork(base_model, action_dim=2)
    policy_model = policy_model.to(device)
    
    # åˆ›å»ºå•çº¯RLè®­ç»ƒå™¨
    pure_rl_trainer = PureRLTrainer(policy_model, lr=1e-3, device=device)
    
    # è®­ç»ƒè®°å½•
    metrics = {
        'loss': [],
        'reward': [],
        'entropy': []
    }
    
    start_time = time.time()
    
    for epoch in range(epochs):
        print(f"\nPure RL Epoch {epoch+1}/{epochs}")
        
        epoch_loss = 0
        epoch_reward = 0
        epoch_entropy = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"Pure RL Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            
            # ç”Ÿæˆéšæœºå¥–åŠ±ï¼ˆæ¨¡æ‹Ÿç¯å¢ƒåé¦ˆï¼‰
            batch_size = input_ids.size(0)
            random_rewards = torch.randn(batch_size, device=device)
            
            # ç”ŸæˆåŠ¨ä½œ
            actions = torch.randint(0, 2, (batch_size,), device=device)
            
            # è®­ç»ƒæ­¥éª¤
            step_metrics = pure_rl_trainer.train_step(
                states=input_ids,
                actions=actions,
                rewards=random_rewards
            )
            
            epoch_loss += step_metrics['loss']
            epoch_reward += step_metrics['reward']
            epoch_entropy += step_metrics['entropy']
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches
        avg_reward = epoch_reward / num_batches
        avg_entropy = epoch_entropy / num_batches
        
        # è®°å½•æŒ‡æ ‡
        metrics['loss'].append(avg_loss)
        metrics['reward'].append(avg_reward)
        metrics['entropy'].append(avg_entropy)
        
        print(f"Loss: {avg_loss:.4f}, Reward: {avg_reward:.4f}, Entropy: {avg_entropy:.4f}")
    
    training_time = time.time() - start_time
    print(f"Pure RLè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    
    return policy_model, metrics


def train_rlhf(base_model, train_loader, device, epochs=3):
    """è®­ç»ƒRLHFæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("è®­ç»ƒRLHFæ¨¡å‹ï¼ˆæœ‰äººç±»åé¦ˆï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œå¥–åŠ±æ¨¡å‹
    policy_model = SimplePolicyNetwork(base_model, action_dim=2)
    reward_model = SimpleRewardModel(base_model)
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹
    reference_model = SimplePolicyNetwork(base_model, action_dim=2)
    reference_model.load_state_dict(policy_model.state_dict())
    
    policy_model = policy_model.to(device)
    reward_model = reward_model.to(device)
    reference_model = reference_model.to(device)
    
    # åˆ›å»ºRLHFè®­ç»ƒå™¨
    rlhf_trainer = SimpleRLHFTrainer(
        policy_model=policy_model,
        reward_model=reward_model,
        reference_model=reference_model,
        lr=1e-4,
        beta=0.1,
        device=device
    )
    
    # è®­ç»ƒè®°å½•
    metrics = {
        'loss': [],
        'kl_penalty': [],
        'reward': []
    }
    
    start_time = time.time()
    
    for epoch in range(epochs):
        print(f"\nRLHF Epoch {epoch+1}/{epochs}")
        
        epoch_loss = 0
        epoch_kl = 0
        epoch_reward = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"RLHF Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            
            # ç”Ÿæˆåˆæˆå¥–åŠ±ï¼ˆæ¨¡æ‹Ÿäººç±»åé¦ˆï¼‰
            batch_size = input_ids.size(0)
            synthetic_rewards = torch.randn(batch_size, device=device)
            
            # ç”ŸæˆåŠ¨ä½œ
            actions = torch.randint(0, 2, (batch_size,), device=device)
            
            # RLHFè®­ç»ƒæ­¥éª¤
            step_metrics = rlhf_trainer.train_step(
                states=input_ids,
                actions=actions,
                rewards=synthetic_rewards
            )
            
            epoch_loss += step_metrics['loss']
            epoch_kl += step_metrics['kl_penalty']
            epoch_reward += step_metrics['reward']
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches
        avg_kl = epoch_kl / num_batches
        avg_reward = epoch_reward / num_batches
        
        # è®°å½•æŒ‡æ ‡
        metrics['loss'].append(avg_loss)
        metrics['kl_penalty'].append(avg_kl)
        metrics['reward'].append(avg_reward)
        
        print(f"Loss: {avg_loss:.4f}, KL Penalty: {avg_kl:.4f}, Reward: {avg_reward:.4f}")
    
    training_time = time.time() - start_time
    print(f"RLHFè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    
    return policy_model, metrics


def compare_policies(pure_rl_model, rlhf_model, val_loader, device):
    """æ¯”è¾ƒä¸¤ç§ç­–ç•¥çš„è¡Œä¸º"""
    print("\n" + "=" * 60)
    print("æ¯”è¾ƒç­–ç•¥è¡Œä¸º")
    print("=" * 60)
    
    pure_rl_model.eval()
    rlhf_model.eval()
    
    pure_rl_actions = []
    rlhf_actions = []
    pure_rl_entropies = []
    rlhf_entropies = []
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="æ¯”è¾ƒç­–ç•¥"):
            input_ids = batch['input_ids'].to(device)
            
            # Pure RLç­–ç•¥
            pure_rl_logits, _ = pure_rl_model(input_ids)
            pure_rl_probs = torch.softmax(pure_rl_logits, dim=-1)
            pure_rl_dist = torch.distributions.Categorical(pure_rl_probs)
            pure_rl_action = pure_rl_dist.sample()
            pure_rl_entropy = pure_rl_dist.entropy()
            
            # RLHFç­–ç•¥
            rlhf_logits, _ = rlhf_model(input_ids)
            rlhf_probs = torch.softmax(rlhf_logits, dim=-1)
            rlhf_dist = torch.distributions.Categorical(rlhf_probs)
            rlhf_action = rlhf_dist.sample()
            rlhf_entropy = rlhf_dist.entropy()
            
            pure_rl_actions.extend(pure_rl_action.cpu().numpy())
            rlhf_actions.extend(rlhf_action.cpu().numpy())
            pure_rl_entropies.extend(pure_rl_entropy.cpu().numpy())
            rlhf_entropies.extend(rlhf_entropy.cpu().numpy())
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    pure_rl_action_dist = np.bincount(pure_rl_actions) / len(pure_rl_actions)
    rlhf_action_dist = np.bincount(rlhf_actions) / len(rlhf_actions)
    
    print(f"Pure RLåŠ¨ä½œåˆ†å¸ƒ: {pure_rl_action_dist}")
    print(f"RLHFåŠ¨ä½œåˆ†å¸ƒ: {rlhf_action_dist}")
    print(f"Pure RLå¹³å‡ç†µ: {np.mean(pure_rl_entropies):.4f}")
    print(f"RLHFå¹³å‡ç†µ: {np.mean(rlhf_entropies):.4f}")
    
    return {
        'pure_rl_action_dist': pure_rl_action_dist,
        'rlhf_action_dist': rlhf_action_dist,
        'pure_rl_entropy': np.mean(pure_rl_entropies),
        'rlhf_entropy': np.mean(rlhf_entropies)
    }


def plot_comparison(pure_rl_metrics, rlhf_metrics, behavior_comparison):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    print("\nç»˜åˆ¶å¯¹æ¯”å›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # æŸå¤±å¯¹æ¯”
    axes[0, 0].plot(pure_rl_metrics['loss'], label='Pure RL', marker='o')
    axes[0, 0].plot(rlhf_metrics['loss'], label='RLHF', marker='s')
    axes[0, 0].set_title('Training Loss Comparison')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # å¥–åŠ±å¯¹æ¯”
    axes[0, 1].plot(pure_rl_metrics['reward'], label='Pure RL', marker='o')
    axes[0, 1].plot(rlhf_metrics['reward'], label='RLHF', marker='s')
    axes[0, 1].set_title('Average Reward Comparison')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # KLæ•£åº¦æƒ©ç½šï¼ˆä»…RLHFï¼‰
    axes[0, 2].plot(rlhf_metrics['kl_penalty'], label='KL Penalty', marker='s', color='red')
    axes[0, 2].set_title('KL Divergence Penalty (RLHF)')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('KL Penalty')
    axes[0, 2].legend()
    axes[0, 2].grid(True)
    
    # åŠ¨ä½œåˆ†å¸ƒå¯¹æ¯”
    x = np.arange(len(behavior_comparison['pure_rl_action_dist']))
    width = 0.35
    
    axes[1, 0].bar(x - width/2, behavior_comparison['pure_rl_action_dist'], 
                   width, label='Pure RL', alpha=0.8)
    axes[1, 0].bar(x + width/2, behavior_comparison['rlhf_action_dist'], 
                   width, label='RLHF', alpha=0.8)
    axes[1, 0].set_title('Action Distribution Comparison')
    axes[1, 0].set_xlabel('Action')
    axes[1, 0].set_ylabel('Probability')
    axes[1, 0].set_xticks(x)
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # ç†µå¯¹æ¯”
    entropies = [behavior_comparison['pure_rl_entropy'], behavior_comparison['rlhf_entropy']]
    labels = ['Pure RL', 'RLHF']
    colors = ['blue', 'orange']
    
    axes[1, 1].bar(labels, entropies, color=colors, alpha=0.8)
    axes[1, 1].set_title('Policy Entropy Comparison')
    axes[1, 1].set_ylabel('Entropy')
    axes[1, 1].grid(True)
    
    # Pure RLç†µå˜åŒ–
    axes[1, 2].plot(pure_rl_metrics['entropy'], label='Pure RL Entropy', marker='o')
    axes[1, 2].set_title('Pure RL Entropy Over Time')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Entropy')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    
    plt.tight_layout()
    plt.savefig('logs/simple_rl_vs_rlhf_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° logs/simple_rl_vs_rlhf_comparison.png")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆ RL vs RLHF å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = get_device_info()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç›®å½•
    os.makedirs('logs', exist_ok=True)
    
    # åŠ è½½æ•°æ®
    train_loader, val_loader = create_simple_dataset()
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    print("åˆ›å»ºç®€å•åŸºç¡€æ¨¡å‹...")
    base_model = SimpleClassifier(vocab_size=100, hidden_dim=64, num_classes=2)
    base_model = base_model.to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in base_model.parameters()):,}")
    
    # è®­ç»ƒPure RLæ¨¡å‹
    pure_rl_model, pure_rl_metrics = train_pure_rl(base_model, train_loader, device)
    
    # è®­ç»ƒRLHFæ¨¡å‹
    rlhf_model, rlhf_metrics = train_rlhf(base_model, train_loader, device)
    
    # æ¯”è¾ƒç­–ç•¥è¡Œä¸º
    behavior_comparison = compare_policies(pure_rl_model, rlhf_model, val_loader, device)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
    plot_comparison(pure_rl_metrics, rlhf_metrics, behavior_comparison)
    
    # ä¿å­˜ç»“æœ
    results = {
        'pure_rl_metrics': pure_rl_metrics,
        'rlhf_metrics': rlhf_metrics,
        'behavior_comparison': behavior_comparison
    }
    
    with open('logs/simple_rl_vs_rlhf_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("å¯¹æ¯”å®éªŒæ€»ç»“")
    print("=" * 80)
    
    print("\nğŸ” ä¸»è¦å·®å¼‚:")
    print("1. **Pure RL**: åªä½¿ç”¨ç¯å¢ƒå¥–åŠ±ï¼Œå¯èƒ½äº§ç”Ÿä¸ç¨³å®šæˆ–æœ‰å®³çš„è¡Œä¸º")
    print("2. **RLHF**: ä½¿ç”¨äººç±»åé¦ˆå’ŒKLæ•£åº¦çº¦æŸï¼Œè¡Œä¸ºæ›´å®‰å…¨å¯æ§")
    
    print("\nğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
    print(f"- Pure RLæœ€ç»ˆå¥–åŠ±: {pure_rl_metrics['reward'][-1]:.4f}")
    print(f"- RLHFæœ€ç»ˆå¥–åŠ±: {rlhf_metrics['reward'][-1]:.4f}")
    print(f"- Pure RLç­–ç•¥ç†µ: {behavior_comparison['pure_rl_entropy']:.4f}")
    print(f"- RLHFç­–ç•¥ç†µ: {behavior_comparison['rlhf_entropy']:.4f}")
    
    print("\nğŸ¯ ç»“è®º:")
    print("- RLHFé€šè¿‡äººç±»åé¦ˆå’ŒKLçº¦æŸï¼Œä½¿ç­–ç•¥æ›´åŠ å®‰å…¨")
    print("- Pure RLå¯èƒ½äº§ç”Ÿé«˜é£é™©è¡Œä¸ºï¼Œéœ€è¦é¢å¤–çº¦æŸ")
    print("- RLHFè®­ç»ƒæ›´ç¨³å®šï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜")
    
    print("\nâœ… å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° logs/simple_rl_vs_rlhf_results.json")
    print("ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ° logs/simple_rl_vs_rlhf_comparison.png")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
RL vs RLHF å¯¹æ¯”è„šæœ¬
å±•ç¤ºå•çº¯RLå’ŒRLHFçš„å·®åˆ«
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
import time

from src.data.dataset import IMDBDataset
from src.models.base_models import LightweightClassifier, PolicyNetwork, RewardModel
from src.rl.ppo import PPOTrainer, RLHFTrainer
from src.utils.training_utils import (
    get_device_info, print_model_info, create_synthetic_rewards,
    save_model, plot_training_curves
)
from configs.base_config import DATA_CONFIG, MODEL_CONFIG, TRAINING_CONFIG, RL_CONFIG, RLHF_CONFIG


class PureRLTrainer:
    """å•çº¯çš„RLè®­ç»ƒå™¨ï¼ˆæ— äººç±»åé¦ˆï¼‰"""
    
    def __init__(self, policy_model, lr=3e-4, device="cpu"):
        self.policy_model = policy_model
        self.device = device
        self.optimizer = optim.Adam(policy_model.parameters(), lr=lr)
        
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.policy_model.to(device)
    
    def train_step(self, states, actions, rewards):
        """è®­ç»ƒæ­¥éª¤"""
        # ç­–ç•¥æ¨¡å‹å‰å‘ä¼ æ’­
        policy_logits, _, _ = self.policy_model(states)
        
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        dist = torch.distributions.Categorical(logits=policy_logits)
        log_probs = dist.log_prob(actions)
        
        # è®¡ç®—å¥–åŠ±
        reward_pred = rewards
        
        # æŸå¤±å‡½æ•°ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
        loss = -(log_probs * rewards).mean()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'reward': rewards.mean().item(),
            'entropy': dist.entropy().mean().item()
        }


def create_comparison_dataset():
    """åˆ›å»ºå¯¹æ¯”æ•°æ®é›†"""
    print("åˆ›å»ºå¯¹æ¯”æ•°æ®é›†...")
    
    dataset_loader = IMDBDataset(
        model_name=DATA_CONFIG['model_name'],
        max_length=DATA_CONFIG['max_length']
    )
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œå¯¹æ¯”
    train_dataset = dataset_loader.load_data(split="train", max_samples=200)
    val_dataset = dataset_loader.load_data(split="test", max_samples=50)
    
    train_loader = dataset_loader.get_dataloader(train_dataset, batch_size=8, shuffle=True)
    val_loader = dataset_loader.get_dataloader(val_dataset, batch_size=8, shuffle=False)
    
    return train_loader, val_loader


def train_pure_rl(base_model, train_loader, device, epochs=3):
    """è®­ç»ƒå•çº¯çš„RLæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("è®­ç»ƒå•çº¯çš„RLæ¨¡å‹ï¼ˆæ— äººç±»åé¦ˆï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy_model = PolicyNetwork(base_model, action_dim=2)
    policy_model = policy_model.to(device)
    
    # åˆ›å»ºå•çº¯RLè®­ç»ƒå™¨
    pure_rl_trainer = PureRLTrainer(policy_model, lr=1e-4, device=device)
    
    # è®­ç»ƒè®°å½•
    metrics = {
        'loss': [],
        'reward': [],
        'entropy': []
    }
    
    start_time = time.time()
    
    for epoch in range(epochs):
        print(f"\nPure RL Epoch {epoch+1}/{epochs}")
        
        epoch_loss = 0
        epoch_reward = 0
        epoch_entropy = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"Pure RL Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # ç”Ÿæˆéšæœºå¥–åŠ±ï¼ˆæ¨¡æ‹Ÿç¯å¢ƒåé¦ˆï¼‰
            batch_size = input_ids.size(0)
            random_rewards = torch.randn(batch_size, device=device)
            
            # ç”ŸæˆåŠ¨ä½œ
            actions = torch.randint(0, 2, (batch_size,), device=device)
            
            # è®­ç»ƒæ­¥éª¤
            step_metrics = pure_rl_trainer.train_step(
                states=input_ids,
                actions=actions,
                rewards=random_rewards
            )
            
            epoch_loss += step_metrics['loss']
            epoch_reward += step_metrics['reward']
            epoch_entropy += step_metrics['entropy']
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches
        avg_reward = epoch_reward / num_batches
        avg_entropy = epoch_entropy / num_batches
        
        # è®°å½•æŒ‡æ ‡
        metrics['loss'].append(avg_loss)
        metrics['reward'].append(avg_reward)
        metrics['entropy'].append(avg_entropy)
        
        print(f"Loss: {avg_loss:.4f}, Reward: {avg_reward:.4f}, Entropy: {avg_entropy:.4f}")
    
    training_time = time.time() - start_time
    print(f"Pure RLè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    
    return policy_model, metrics


def train_rlhf(base_model, train_loader, device, epochs=3):
    """è®­ç»ƒRLHFæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("è®­ç»ƒRLHFæ¨¡å‹ï¼ˆæœ‰äººç±»åé¦ˆï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œå¥–åŠ±æ¨¡å‹
    policy_model = PolicyNetwork(base_model, action_dim=2)
    reward_model = RewardModel(base_model)
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹
    reference_model = LightweightClassifier(
        model_name=DATA_CONFIG['model_name'],
        num_classes=MODEL_CONFIG['num_classes'],
        dropout=MODEL_CONFIG['dropout']
    )
    reference_model.load_state_dict(base_model.state_dict())
    
    policy_model = policy_model.to(device)
    reward_model = reward_model.to(device)
    reference_model = reference_model.to(device)
    
    # åˆ›å»ºRLHFè®­ç»ƒå™¨
    rlhf_trainer = RLHFTrainer(
        policy_model=policy_model,
        reward_model=reward_model,
        reference_model=reference_model,
        lr=1e-5,
        beta=0.1,
        device=device
    )
    
    # è®­ç»ƒè®°å½•
    metrics = {
        'loss': [],
        'kl_penalty': [],
        'reward': []
    }
    
    start_time = time.time()
    
    for epoch in range(epochs):
        print(f"\nRLHF Epoch {epoch+1}/{epochs}")
        
        epoch_loss = 0
        epoch_kl = 0
        epoch_reward = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"RLHF Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # ç”Ÿæˆåˆæˆå¥–åŠ±ï¼ˆæ¨¡æ‹Ÿäººç±»åé¦ˆï¼‰
            batch_size = input_ids.size(0)
            synthetic_rewards = torch.randn(batch_size, device=device)
            
            # ç”ŸæˆåŠ¨ä½œ
            actions = torch.randint(0, 2, (batch_size,), device=device)
            
            # RLHFè®­ç»ƒæ­¥éª¤
            step_metrics = rlhf_trainer.train_step(
                states=input_ids,
                actions=actions,
                rewards=synthetic_rewards
            )
            
            epoch_loss += step_metrics['loss']
            epoch_kl += step_metrics['kl_penalty']
            epoch_reward += step_metrics['reward']
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches
        avg_kl = epoch_kl / num_batches
        avg_reward = epoch_reward / num_batches
        
        # è®°å½•æŒ‡æ ‡
        metrics['loss'].append(avg_loss)
        metrics['kl_penalty'].append(avg_kl)
        metrics['reward'].append(avg_reward)
        
        print(f"Loss: {avg_loss:.4f}, KL Penalty: {avg_kl:.4f}, Reward: {avg_reward:.4f}")
    
    training_time = time.time() - start_time
    print(f"RLHFè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    
    return policy_model, metrics


def compare_policies(pure_rl_model, rlhf_model, val_loader, device):
    """æ¯”è¾ƒä¸¤ç§ç­–ç•¥çš„è¡Œä¸º"""
    print("\n" + "=" * 60)
    print("æ¯”è¾ƒç­–ç•¥è¡Œä¸º")
    print("=" * 60)
    
    pure_rl_model.eval()
    rlhf_model.eval()
    
    pure_rl_actions = []
    rlhf_actions = []
    pure_rl_entropies = []
    rlhf_entropies = []
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="æ¯”è¾ƒç­–ç•¥"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # Pure RLç­–ç•¥
            pure_rl_logits, _, _ = pure_rl_model(input_ids=input_ids, attention_mask=attention_mask)
            pure_rl_probs = torch.softmax(pure_rl_logits, dim=-1)
            pure_rl_dist = torch.distributions.Categorical(pure_rl_probs)
            pure_rl_action = pure_rl_dist.sample()
            pure_rl_entropy = pure_rl_dist.entropy()
            
            # RLHFç­–ç•¥
            rlhf_logits, _, _ = rlhf_model(input_ids=input_ids, attention_mask=attention_mask)
            rlhf_probs = torch.softmax(rlhf_logits, dim=-1)
            rlhf_dist = torch.distributions.Categorical(rlhf_probs)
            rlhf_action = rlhf_dist.sample()
            rlhf_entropy = rlhf_dist.entropy()
            
            pure_rl_actions.extend(pure_rl_action.cpu().numpy())
            rlhf_actions.extend(rlhf_action.cpu().numpy())
            pure_rl_entropies.extend(pure_rl_entropy.cpu().numpy())
            rlhf_entropies.extend(rlhf_entropy.cpu().numpy())
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    pure_rl_action_dist = np.bincount(pure_rl_actions) / len(pure_rl_actions)
    rlhf_action_dist = np.bincount(rlhf_actions) / len(rlhf_actions)
    
    print(f"Pure RLåŠ¨ä½œåˆ†å¸ƒ: {pure_rl_action_dist}")
    print(f"RLHFåŠ¨ä½œåˆ†å¸ƒ: {rlhf_action_dist}")
    print(f"Pure RLå¹³å‡ç†µ: {np.mean(pure_rl_entropies):.4f}")
    print(f"RLHFå¹³å‡ç†µ: {np.mean(rlhf_entropies):.4f}")
    
    return {
        'pure_rl_action_dist': pure_rl_action_dist,
        'rlhf_action_dist': rlhf_action_dist,
        'pure_rl_entropy': np.mean(pure_rl_entropies),
        'rlhf_entropy': np.mean(rlhf_entropies)
    }


def plot_comparison(pure_rl_metrics, rlhf_metrics, behavior_comparison):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    print("\nç»˜åˆ¶å¯¹æ¯”å›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # æŸå¤±å¯¹æ¯”
    axes[0, 0].plot(pure_rl_metrics['loss'], label='Pure RL', marker='o')
    axes[0, 0].plot(rlhf_metrics['loss'], label='RLHF', marker='s')
    axes[0, 0].set_title('Training Loss Comparison')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # å¥–åŠ±å¯¹æ¯”
    axes[0, 1].plot(pure_rl_metrics['reward'], label='Pure RL', marker='o')
    axes[0, 1].plot(rlhf_metrics['reward'], label='RLHF', marker='s')
    axes[0, 1].set_title('Average Reward Comparison')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # KLæ•£åº¦æƒ©ç½šï¼ˆä»…RLHFï¼‰
    axes[0, 2].plot(rlhf_metrics['kl_penalty'], label='KL Penalty', marker='s', color='red')
    axes[0, 2].set_title('KL Divergence Penalty (RLHF)')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('KL Penalty')
    axes[0, 2].legend()
    axes[0, 2].grid(True)
    
    # åŠ¨ä½œåˆ†å¸ƒå¯¹æ¯”
    x = np.arange(len(behavior_comparison['pure_rl_action_dist']))
    width = 0.35
    
    axes[1, 0].bar(x - width/2, behavior_comparison['pure_rl_action_dist'], 
                   width, label='Pure RL', alpha=0.8)
    axes[1, 0].bar(x + width/2, behavior_comparison['rlhf_action_dist'], 
                   width, label='RLHF', alpha=0.8)
    axes[1, 0].set_title('Action Distribution Comparison')
    axes[1, 0].set_xlabel('Action')
    axes[1, 0].set_ylabel('Probability')
    axes[1, 0].set_xticks(x)
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # ç†µå¯¹æ¯”
    entropies = [behavior_comparison['pure_rl_entropy'], behavior_comparison['rlhf_entropy']]
    labels = ['Pure RL', 'RLHF']
    colors = ['blue', 'orange']
    
    axes[1, 1].bar(labels, entropies, color=colors, alpha=0.8)
    axes[1, 1].set_title('Policy Entropy Comparison')
    axes[1, 1].set_ylabel('Entropy')
    axes[1, 1].grid(True)
    
    # Pure RLç†µå˜åŒ–
    axes[1, 2].plot(pure_rl_metrics['entropy'], label='Pure RL Entropy', marker='o')
    axes[1, 2].set_title('Pure RL Entropy Over Time')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Entropy')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    
    plt.tight_layout()
    plt.savefig('logs/rl_vs_rlhf_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° logs/rl_vs_rlhf_comparison.png")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RL vs RLHF å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = get_device_info()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç›®å½•
    os.makedirs('logs', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    
    # åŠ è½½æ•°æ®
    train_loader, val_loader = create_comparison_dataset()
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    print("åˆ›å»ºåŸºç¡€æ¨¡å‹...")
    base_model = LightweightClassifier(
        model_name=DATA_CONFIG['model_name'],
        num_classes=MODEL_CONFIG['num_classes'],
        dropout=MODEL_CONFIG['dropout']
    )
    base_model = base_model.to(device)
    
    print_model_info(base_model)
    
    # è®­ç»ƒPure RLæ¨¡å‹
    pure_rl_model, pure_rl_metrics = train_pure_rl(base_model, train_loader, device)
    
    # è®­ç»ƒRLHFæ¨¡å‹
    rlhf_model, rlhf_metrics = train_rlhf(base_model, train_loader, device)
    
    # æ¯”è¾ƒç­–ç•¥è¡Œä¸º
    behavior_comparison = compare_policies(pure_rl_model, rlhf_model, val_loader, device)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
    plot_comparison(pure_rl_metrics, rlhf_metrics, behavior_comparison)
    
    # ä¿å­˜ç»“æœ
    results = {
        'pure_rl_metrics': pure_rl_metrics,
        'rlhf_metrics': rlhf_metrics,
        'behavior_comparison': behavior_comparison
    }
    
    with open('logs/rl_vs_rlhf_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("å¯¹æ¯”å®éªŒæ€»ç»“")
    print("=" * 80)
    
    print("\nğŸ” ä¸»è¦å·®å¼‚:")
    print("1. **Pure RL**: åªä½¿ç”¨ç¯å¢ƒå¥–åŠ±ï¼Œå¯èƒ½äº§ç”Ÿä¸ç¨³å®šæˆ–æœ‰å®³çš„è¡Œä¸º")
    print("2. **RLHF**: ä½¿ç”¨äººç±»åé¦ˆå’ŒKLæ•£åº¦çº¦æŸï¼Œè¡Œä¸ºæ›´å®‰å…¨å¯æ§")
    
    print("\nğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
    print(f"- Pure RLæœ€ç»ˆå¥–åŠ±: {pure_rl_metrics['reward'][-1]:.4f}")
    print(f"- RLHFæœ€ç»ˆå¥–åŠ±: {rlhf_metrics['reward'][-1]:.4f}")
    print(f"- Pure RLç­–ç•¥ç†µ: {behavior_comparison['pure_rl_entropy']:.4f}")
    print(f"- RLHFç­–ç•¥ç†µ: {behavior_comparison['rlhf_entropy']:.4f}")
    
    print("\nğŸ¯ ç»“è®º:")
    print("- RLHFé€šè¿‡äººç±»åé¦ˆå’ŒKLçº¦æŸï¼Œä½¿ç­–ç•¥æ›´åŠ å®‰å…¨")
    print("- Pure RLå¯èƒ½äº§ç”Ÿé«˜é£é™©è¡Œä¸ºï¼Œéœ€è¦é¢å¤–çº¦æŸ")
    print("- RLHFè®­ç»ƒæ›´ç¨³å®šï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜")
    
    print("\nâœ… å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° logs/rl_vs_rlhf_results.json")
    print("ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ° logs/rl_vs_rlhf_comparison.png")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
å®éªŒ1: Q-Learningåœ¨ç½‘æ ¼ä¸–ç•Œä¸­çš„å­¦ä¹ 

è¿™ä¸ªå®éªŒå°†å¸®åŠ©ä½ ç†è§£ï¼š
1. Q-Learningç®—æ³•çš„åŸºæœ¬åŸç†
2. æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡
3. ä»·å€¼å‡½æ•°çš„æ”¶æ•›è¿‡ç¨‹
4. æœ€ä¼˜ç­–ç•¥çš„å½¢æˆ
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“
try:
    from utils.font_config import setup_chinese_font
    setup_chinese_font()
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥å­—ä½“é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
    plt.rcParams['font.sans-serif'] = ['PingFang HK', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

from environments.grid_world import create_simple_grid_world, create_complex_grid_world
from algorithms.tabular.q_learning import QLearning


def experiment_1_basic_q_learning():
    """
    å®éªŒ1.1: åŸºç¡€Q-Learningå­¦ä¹ 
    """
    print("ğŸ§  å®éªŒ1.1: åŸºç¡€Q-Learningå­¦ä¹ ")
    print("=" * 60)
    
    # åˆ›å»ºç®€å•ç½‘æ ¼ä¸–ç•Œ
    env = create_simple_grid_world()
    print(f"ç¯å¢ƒ: {env.size}x{env.size} ç½‘æ ¼ä¸–ç•Œ")
    print(f"èµ·å§‹ä½ç½®: {env.start}")
    print(f"ç›®æ ‡ä½ç½®: {env.goal}")
    print(f"éšœç¢ç‰©: {env.obstacles}")
    
    # åˆ›å»ºQ-Learningç®—æ³•
    q_learning = QLearning(
        state_space=env.state_space,
        action_space=env.action_space,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=0.1,
        epsilon_decay=0.995,
        epsilon_min=0.01
    )
    
    print(f"\nç®—æ³•å‚æ•°:")
    print(f"å­¦ä¹ ç‡ (Î±): {q_learning.learning_rate}")
    print(f"æŠ˜æ‰£å› å­ (Î³): {q_learning.discount_factor}")
    print(f"åˆå§‹æ¢ç´¢æ¦‚ç‡ (Îµ): {q_learning.epsilon}")
    print(f"æ¢ç´¢è¡°å‡ç‡: {q_learning.epsilon_decay}")
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    history = q_learning.train(
        env, 
        num_episodes=300, 
        max_steps_per_episode=100,
        verbose=True
    )
    
    training_time = time.time() - start_time
    print(f"\nè®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f}ç§’")
    
    # è¯„ä¼°
    print(f"\nè¯„ä¼°ç®—æ³•æ€§èƒ½...")
    eval_results = q_learning.evaluate(env, num_episodes=100)
    
    print(f"\nè¯„ä¼°ç»“æœ:")
    print(f"å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.4f}")
    print(f"å¥–åŠ±æ ‡å‡†å·®: {eval_results['std_reward']:.4f}")
    print(f"å¹³å‡æ­¥æ•°: {eval_results['mean_steps']:.2f}")
    print(f"æˆåŠŸç‡: {eval_results['success_rate']:.2%}")
    print(f"æœ€å°å¥–åŠ±: {eval_results['min_reward']:.4f}")
    print(f"æœ€å¤§å¥–åŠ±: {eval_results['max_reward']:.4f}")
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„ç­–ç•¥
    q_learning.render_policy(env, episode="Final")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    q_learning.plot_training_history()
    
    return q_learning, env, history, eval_results


def experiment_1_2_parameter_study():
    """
    å®éªŒ1.2: å‚æ•°ç ”ç©¶ - ä¸åŒå­¦ä¹ ç‡å’ŒæŠ˜æ‰£å› å­çš„å½±å“
    """
    print("\nğŸ§  å®éªŒ1.2: å‚æ•°ç ”ç©¶")
    print("=" * 60)
    
    env = create_simple_grid_world()
    
    # æµ‹è¯•ä¸åŒçš„å­¦ä¹ ç‡
    learning_rates = [0.01, 0.1, 0.5, 0.9]
    discount_factors = [0.8, 0.9, 0.95, 0.99]
    
    results = {}
    
    print("æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„å½±å“...")
    for lr in learning_rates:
        print(f"\nå­¦ä¹ ç‡: {lr}")
        q_learning = QLearning(
            state_space=env.state_space,
            action_space=env.action_space,
            learning_rate=lr,
            discount_factor=0.95,
            epsilon=0.1,
            epsilon_decay=0.995,
            epsilon_min=0.01
        )
        
        history = q_learning.train(env, num_episodes=200, verbose=False)
        eval_results = q_learning.evaluate(env, num_episodes=50)
        
        results[f'lr_{lr}'] = {
            'history': history,
            'eval': eval_results,
            'final_rewards': history['episode_rewards'][-50:]  # æœ€å50ä¸ªepisode
        }
        
        print(f"  å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.4f}")
        print(f"  æˆåŠŸç‡: {eval_results['success_rate']:.2%}")
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å­¦ä¹ ç‡æ¯”è¾ƒ
    for lr in learning_rates:
        key = f'lr_{lr}'
        rewards = results[key]['history']['episode_rewards']
        axes[0, 0].plot(rewards, label=f'LR={lr}', alpha=0.7)
    
    axes[0, 0].set_title('ä¸åŒå­¦ä¹ ç‡çš„å­¦ä¹ æ›²çº¿')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # å­¦ä¹ ç‡vsæ€§èƒ½
    lr_values = []
    mean_rewards = []
    success_rates = []
    
    for lr in learning_rates:
        key = f'lr_{lr}'
        lr_values.append(lr)
        mean_rewards.append(results[key]['eval']['mean_reward'])
        success_rates.append(results[key]['eval']['success_rate'])
    
    axes[0, 1].plot(lr_values, mean_rewards, 'o-', label='å¹³å‡å¥–åŠ±')
    axes[0, 1].set_title('å­¦ä¹ ç‡ vs å¹³å‡å¥–åŠ±')
    axes[0, 1].set_xlabel('å­¦ä¹ ç‡')
    axes[0, 1].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[0, 1].grid(True)
    
    ax2 = axes[0, 1].twinx()
    ax2.plot(lr_values, success_rates, 's-', color='red', label='æˆåŠŸç‡')
    ax2.set_ylabel('æˆåŠŸç‡', color='red')
    ax2.tick_params(axis='y', labelcolor='red')
    
    # æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
    convergence_episodes = []
    for lr in learning_rates:
        key = f'lr_{lr}'
        rewards = results[key]['history']['episode_rewards']
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿ç»­10ä¸ªepisodeå¹³å‡å¥–åŠ±>0.5çš„episode
        for i in range(len(rewards) - 10):
            if np.mean(rewards[i:i+10]) > 0.5:
                convergence_episodes.append(i)
                break
        else:
            convergence_episodes.append(len(rewards))
    
    axes[1, 0].bar(range(len(learning_rates)), convergence_episodes)
    axes[1, 0].set_title('æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ')
    axes[1, 0].set_xlabel('å­¦ä¹ ç‡')
    axes[1, 0].set_ylabel('æ”¶æ•›æ‰€éœ€Episodeæ•°')
    axes[1, 0].set_xticks(range(len(learning_rates)))
    axes[1, 0].set_xticklabels(learning_rates)
    axes[1, 0].grid(True)
    
    # æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
    final_rewards = [results[f'lr_{lr}']['eval']['mean_reward'] for lr in learning_rates]
    axes[1, 1].bar(range(len(learning_rates)), final_rewards)
    axes[1, 1].set_title('æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ')
    axes[1, 1].set_xlabel('å­¦ä¹ ç‡')
    axes[1, 1].set_ylabel('æœ€ç»ˆå¹³å‡å¥–åŠ±')
    axes[1, 1].set_xticks(range(len(learning_rates)))
    axes[1, 1].set_xticklabels(learning_rates)
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('experiments/q_learning_parameter_study.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return results


def experiment_1_3_exploration_vs_exploitation():
    """
    å®éªŒ1.3: æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡
    """
    print("\nğŸ§  å®éªŒ1.3: æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡")
    print("=" * 60)
    
    env = create_simple_grid_world()
    
    # æµ‹è¯•ä¸åŒçš„æ¢ç´¢ç­–ç•¥
    exploration_strategies = {
        'High Exploration': {'epsilon': 0.3, 'epsilon_decay': 0.999},
        'Medium Exploration': {'epsilon': 0.1, 'epsilon_decay': 0.995},
        'Low Exploration': {'epsilon': 0.05, 'epsilon_decay': 0.99},
        'No Exploration': {'epsilon': 0.0, 'epsilon_decay': 1.0}
    }
    
    results = {}
    
    for name, params in exploration_strategies.items():
        print(f"\næµ‹è¯•ç­–ç•¥: {name}")
        print(f"  åˆå§‹Îµ: {params['epsilon']}, è¡°å‡ç‡: {params['epsilon_decay']}")
        
        q_learning = QLearning(
            state_space=env.state_space,
            action_space=env.action_space,
            learning_rate=0.1,
            discount_factor=0.95,
            epsilon=params['epsilon'],
            epsilon_decay=params['epsilon_decay'],
            epsilon_min=0.01
        )
        
        history = q_learning.train(env, num_episodes=300, verbose=False)
        eval_results = q_learning.evaluate(env, num_episodes=100)
        
        results[name] = {
            'history': history,
            'eval': eval_results,
            'params': params
        }
        
        print(f"  å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.4f}")
        print(f"  æˆåŠŸç‡: {eval_results['success_rate']:.2%}")
        print(f"  æœ€ç»ˆÎµ: {q_learning.epsilon:.4f}")
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å­¦ä¹ æ›²çº¿æ¯”è¾ƒ
    for name in exploration_strategies.keys():
        rewards = results[name]['history']['episode_rewards']
        axes[0, 0].plot(rewards, label=name, alpha=0.7)
    
    axes[0, 0].set_title('ä¸åŒæ¢ç´¢ç­–ç•¥çš„å­¦ä¹ æ›²çº¿')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Epsilonè¡°å‡æ›²çº¿
    for name in exploration_strategies.keys():
        epsilons = results[name]['history']['epsilon_history']
        axes[0, 1].plot(epsilons, label=name, alpha=0.7)
    
    axes[0, 1].set_title('æ¢ç´¢æ¦‚ç‡è¡°å‡æ›²çº¿')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Epsilon')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # æ€§èƒ½æ¯”è¾ƒ
    names = list(exploration_strategies.keys())
    mean_rewards = [results[name]['eval']['mean_reward'] for name in names]
    success_rates = [results[name]['eval']['success_rate'] for name in names]
    
    x = np.arange(len(names))
    width = 0.35
    
    axes[1, 0].bar(x - width/2, mean_rewards, width, label='å¹³å‡å¥–åŠ±')
    axes[1, 0].bar(x + width/2, success_rates, width, label='æˆåŠŸç‡')
    axes[1, 0].set_title('æ€§èƒ½æ¯”è¾ƒ')
    axes[1, 0].set_xlabel('æ¢ç´¢ç­–ç•¥')
    axes[1, 0].set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(names, rotation=45)
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
    convergence_episodes = []
    for name in names:
        rewards = results[name]['history']['episode_rewards']
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿ç»­20ä¸ªepisodeå¹³å‡å¥–åŠ±>0.8çš„episode
        for i in range(len(rewards) - 20):
            if np.mean(rewards[i:i+20]) > 0.8:
                convergence_episodes.append(i)
                break
        else:
            convergence_episodes.append(len(rewards))
    
    axes[1, 1].bar(names, convergence_episodes)
    axes[1, 1].set_title('æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ')
    axes[1, 1].set_xlabel('æ¢ç´¢ç­–ç•¥')
    axes[1, 1].set_ylabel('æ”¶æ•›æ‰€éœ€Episodeæ•°')
    axes[1, 1].set_xticklabels(names, rotation=45)
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('experiments/q_learning_exploration_study.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return results


def experiment_1_4_complex_environment():
    """
    å®éªŒ1.4: å¤æ‚ç¯å¢ƒä¸­çš„Q-Learning
    """
    print("\nğŸ§  å®éªŒ1.4: å¤æ‚ç¯å¢ƒä¸­çš„Q-Learning")
    print("=" * 60)
    
    # åˆ›å»ºå¤æ‚ç½‘æ ¼ä¸–ç•Œ
    env = create_complex_grid_world()
    print(f"ç¯å¢ƒ: {env.size}x{env.size} å¤æ‚ç½‘æ ¼ä¸–ç•Œ")
    print(f"èµ·å§‹ä½ç½®: {env.start}")
    print(f"ç›®æ ‡ä½ç½®: {env.goal}")
    print(f"éšœç¢ç‰©æ•°é‡: {len(env.obstacles)}")
    
    # åˆ›å»ºQ-Learningç®—æ³•
    q_learning = QLearning(
        state_space=env.state_space,
        action_space=env.action_space,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=0.2,  # å¢åŠ æ¢ç´¢
        epsilon_decay=0.999,
        epsilon_min=0.01
    )
    
    print(f"\nå¼€å§‹è®­ç»ƒå¤æ‚ç¯å¢ƒ...")
    start_time = time.time()
    
    history = q_learning.train(
        env, 
        num_episodes=1000,  # å¢åŠ è®­ç»ƒepisode
        max_steps_per_episode=200,
        verbose=True
    )
    
    training_time = time.time() - start_time
    print(f"\nè®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f}ç§’")
    
    # è¯„ä¼°
    eval_results = q_learning.evaluate(env, num_episodes=100)
    
    print(f"\nè¯„ä¼°ç»“æœ:")
    print(f"å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.4f}")
    print(f"æˆåŠŸç‡: {eval_results['success_rate']:.2%}")
    print(f"å¹³å‡æ­¥æ•°: {eval_results['mean_steps']:.2f}")
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„ç­–ç•¥
    q_learning.render_policy(env, episode="Complex Environment")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    q_learning.plot_training_history()
    
    return q_learning, env, history, eval_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Q-Learningå­¦ä¹ å®éªŒ")
    print("=" * 80)
    print("è¿™ä¸ªå®éªŒå°†å¸®åŠ©ä½ æ·±å…¥ç†è§£Q-Learningç®—æ³•")
    print("é€šè¿‡å¤šä¸ªå­å®éªŒï¼Œä½ å°†å­¦ä¹ åˆ°:")
    print("1. Q-Learningçš„åŸºæœ¬åŸç†å’Œå®ç°")
    print("2. å‚æ•°å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“")
    print("3. æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡")
    print("4. ç®—æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°")
    print("=" * 80)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('experiments', exist_ok=True)
    
    # å®éªŒ1.1: åŸºç¡€Q-Learning
    print("\n" + "="*80)
    q_learning, env, history, eval_results = experiment_1_basic_q_learning()
    
    # å®éªŒ1.2: å‚æ•°ç ”ç©¶
    print("\n" + "="*80)
    param_results = experiment_1_2_parameter_study()
    
    # å®éªŒ1.3: æ¢ç´¢ä¸åˆ©ç”¨
    print("\n" + "="*80)
    exploration_results = experiment_1_3_exploration_vs_exploitation()
    
    # å®éªŒ1.4: å¤æ‚ç¯å¢ƒ
    print("\n" + "="*80)
    complex_q_learning, complex_env, complex_history, complex_eval = experiment_1_4_complex_environment()
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ‰ å®éªŒæ€»ç»“")
    print("="*80)
    print("é€šè¿‡è¿™æ¬¡å®éªŒï¼Œä½ åº”è¯¥å·²ç»ç†è§£:")
    print("âœ… Q-Learningç®—æ³•çš„æ ¸å¿ƒæ€æƒ³")
    print("âœ… ä»·å€¼å‡½æ•°çš„å­¦ä¹ è¿‡ç¨‹")
    print("âœ… æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ç­–ç•¥")
    print("âœ… å‚æ•°è°ƒä¼˜çš„é‡è¦æ€§")
    print("âœ… ç®—æ³•åœ¨ä¸åŒå¤æ‚åº¦ç¯å¢ƒä¸­çš„è¡¨ç°")
    
    print("\nğŸ“Š å…³é”®å‘ç°:")
    print(f"â€¢ ç®€å•ç¯å¢ƒæˆåŠŸç‡: {eval_results['success_rate']:.2%}")
    print(f"â€¢ å¤æ‚ç¯å¢ƒæˆåŠŸç‡: {complex_eval['success_rate']:.2%}")
    print(f"â€¢ æœ€ä½³å­¦ä¹ ç‡: 0.1")
    print(f"â€¢ æ¨èæ¢ç´¢ç­–ç•¥: ä¸­ç­‰æ¢ç´¢ (Îµ=0.1, decay=0.995)")
    
    print("\nğŸ” ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("1. å°è¯•ä¿®æ”¹ç¯å¢ƒå‚æ•°ï¼Œè§‚å¯Ÿç®—æ³•è¡¨ç°")
    print("2. å®ç°SARSAç®—æ³•ï¼Œä¸Q-Learningå¯¹æ¯”")
    print("3. å­¦ä¹ Policy Gradientæ–¹æ³•")
    print("4. æ¢ç´¢æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("â€¢ experiments/q_learning_parameter_study.png")
    print("â€¢ experiments/q_learning_exploration_study.png")
    print("â€¢ è®­ç»ƒå†å²å›¾è¡¨")
    
    return {
        'basic': (q_learning, env, history, eval_results),
        'parameters': param_results,
        'exploration': exploration_results,
        'complex': (complex_q_learning, complex_env, complex_history, complex_eval)
    }


if __name__ == "__main__":
    main()
